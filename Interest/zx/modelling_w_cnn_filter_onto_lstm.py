# -*- coding: utf-8 -*-
"""modelling w CNN filter onto LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rMnHCWAuyznujjLZoK8JdvyGnSWAre5x
"""

## install and import libraries
!pip install -U -q PyDrive
import pandas as pd
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

## authentication to Google Drive
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

## find all files within your own Google Drive
listed = drive.ListFile({'q': "title contains '.csv' or title contains '50d.txt'"}).GetList()
for file in listed:
  print('title {}, id {}'.format(file['title'], file['id']))

# jy's gdrive
# train_id = '1qKKSEvxF5MM24L9VTLhc9IX_V2hmD6v7'
# test_id = '1wpKObBGwqYgfUKy2Ou3AcJY1_R_ugHZN'

# zx's
train_id = '1k5PIJZi3FPcaoALHyCZlLLjfTONhgUC4'
test_id = '1gxSe-LN8cJnMpTC1-WegfUV7U5jPvWFr'
embeddings = '1dPqXUz2EYdhwBt_xrBuKkUDzVsInWfKm'

download_train = drive.CreateFile({'id': train_id})
download_test = drive.CreateFile({'id': test_id})
download_embeddings = drive.CreateFile({'id': embeddings})

download_train.GetContentFile('train.csv')
download_test.GetContentFile('test.csv')
download_embeddings.GetContentFile('glove.twitter.27B.50d.txt')

# list files on colab drive
!ls

"""## Train test split"""

# get relevant columns - male 0, female 1
test = pd.read_csv('test.csv').loc[:, ['tweets', 'age', 'gender']]
train = pd.read_csv('train.csv').loc[:, ['tweets', 'age', 'gender']]
df = pd.concat([test, train])

# for gender model
train_X, train_y = df['tweets'], df['age']

print(train_X.shape)
print(train_y.shape)

train.age.describe()

def age_bins(age):
  if age < 25:
    return 'Young'
  elif 25 <= age <= 50:
    return 'Middle Aged'
  elif age > 50:
    return 'Elderly'
  
# for age model
train_y_age_bins = pd.get_dummies(train_y.apply(age_bins))

print(train_y_age_bins.shape)

"""## Tokenize and create sequence

Source: https://medium.com/@sabber/classifying-yelp-review-comments-using-cnn-lstm-and-pre-trained-glove-word-embeddings-part-3-53fcea9a17fa
"""

tokenizer = Tokenizer(num_words=30000)

# fit tokenizer on train and test sets
tokenizer.fit_on_texts(df['tweets'])

# create dtm 
train_X_dtm = pad_sequences(tokenizer.texts_to_sequences(train_X), maxlen=5000)

vocabulary_size = len(tokenizer.word_counts)
print(vocabulary_size)
print(train_X_dtm.shape)

"""## Modelling"""

# import remaining libraries
from keras.models import Sequential
from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation
from keras.layers.embeddings import Embedding
## Plotly
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
# Others
import nltk
import string
import pickle
import numpy as np
import pandas as pd
from nltk.corpus import stopwords

from sklearn.manifold import TSNE

## load embeddings 
embeddings_index = dict()
f = open('glove.twitter.27B.50d.txt')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

## build embedding matrix
embedding_matrix = np.zeros((vocabulary_size, 50))
for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[index] = embedding_vector[:50]

print(embedding_matrix.shape)

which = 'age' ### change this to gender settle everything

with open('tokenizer30{}.pickle'.format(which), 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

with open('embeddings30{}.pickle'.format(which), 'wb') as handle:
    pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)

## create model
model_glove = Sequential()
model_glove.add(Embedding(vocabulary_size, 50, input_length=5000, \
                          weights=[embedding_matrix], trainable=False))
model_glove.add(Dropout(0.2))
model_glove.add(Conv1D(64, 5, activation='relu'))
model_glove.add(MaxPooling1D(pool_size=4))
model_glove.add(LSTM(100))
if which == 'age':
  model_glove.add(Dense(3, activation='softmax'))
elif which == 'gender':
  model_glove.add(Dense(1, activation='sigmoid')) # train for age
model_glove.compile(loss='binary_crossentropy', \
                    optimizer='adam', metrics=['accuracy'])

## Fit train data
model_glove.fit(train_X_dtm, train_y_age_bins, validation_split=0.1, epochs = 30)

# https://jovianlin.io/saving-loading-keras-models/
from keras.models import load_model

# Creates a HDF5 file 'gender_model.h5'
model.save('bestmodel{}.h5'.format(which))

from google.colab import files

for file in ['tokenizer30{}.pickle', \
               'embeddings30{}.pickle', 'bestmodel{}.h5']:
  uploaded = drive.CreateFile({'title': file.format(which)})
  uploaded.SetContentString(file.format(which))
  uploaded.Upload()
  print('Uploaded file with ID {}'.format(uploaded.get('id')))

"""
TO-CHECK: functions to be placed in Dion's script 
"""

def predictUser(user,numTweets=500, model='interest'): # defaults to Dion's 
    userTweets = extractTweets(user,numTweets) # input number of tweets to pull as desired (>= 200)
    userTweets = processTweets(userTweets)
    data = tokenizer.texts_to_sequences(userTweets)
    data = sequence.pad_sequences(data, maxlen = maxTweetLength, padding = 'post')
    results = model.predict(data)
    results = np.argmax(results,axis=1)
    if model == 'interest':
      interest_key = Counter(results)
      interest_key = interest_key.most_common(1)[0][0]
      interest = predictionDic.get(interest_key)
      return interest
    elif model == 'age':
      return age
    elif model == 'gender':
      return gender
    
def loadCNN(model): # empty string takes dion's by default
  global tokenizer
    with open('tokenizer30.pickle{}'.format(model), 'rb') as handle:
        tokenizer = pickle.load(handle)
    tokenizer.oov_token = None
    global embeddings_matrix    
    with open('embeddings30.pickle{}'.format(model), 'rb') as handle:
        embeddings_matrix = pickle.load(handle)
    global model
  model = load_model('bestmodel{}.h5')
  
"""
TO-DO: get the embeddings, tokenizer, & best model for age. (JINGYU help pls)
"""